{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a04bb33",
   "metadata": {},
   "source": [
    "## CSCI 316 Indivual Assignment 2 Task 1\n",
    "### Dataset information\n",
    "##### The wordsList file contains 72 pre-processed emails. Each line is a list of words extracted from each email.\n",
    "##### The classList file contains the class labels that indicating whether the emails are ordinary or adverts (0 forordinary and 1 for adverts).\n",
    "### Objective\n",
    "##### Develop a Naïve Bayesian classifier as an email filter in Python. Namely, the classifier predicts whether emails are ordinary or adverts.\n",
    "### Task requirements\n",
    "##### (1) Use stratified sampling to select 66 out of 72 lines for training and the remaining 6 lines for test. Return the classification probabilities of these 6 records.\n",
    "##### (2) The Naïve Bayesian classifier must account for multiple occurrences of words and implementstechniques to overcome the numerical underflows and zero counts.\n",
    "##### (3) No ML library can be used in this task. The implementation must be developed from scratch.However, scientific computing libraries such as NumPy and SciPy are allowed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e340ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries needed for this task \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22e4b34",
   "metadata": {},
   "source": [
    "### Reading and importing wordsList and classList file as a csv file, and adding classList into wordsList df. So wordsList has wordList column and classList column which contains whetther the emails are ordinary or adverts(0 for ordinary and 1 for adverts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "38cc2a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wordList</th>\n",
       "      <th>classList</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>codeine,15mg,for,203,visa,only,codeine,methylm...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>peter,with,jose,out,town,you,want,meet,once,wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hydrocodone,vicodin,brand,watson,vicodin,750,1...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>yay,you,both,doing,fine,working,mba,design,str...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>you,have,everything,gain,incredib1e,gains,leng...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>scifinance,now,automatically,generates,gpu,ena...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>you,have,everything,gain,incredib1e,gains,leng...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>will,there,the,latest</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>experience,with,biggerpenis,today,grow,inches,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>that,cold,there,going,retirement,party,are,the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             wordList  classList\n",
       "0   codeine,15mg,for,203,visa,only,codeine,methylm...          1\n",
       "1   peter,with,jose,out,town,you,want,meet,once,wh...          0\n",
       "2   hydrocodone,vicodin,brand,watson,vicodin,750,1...          1\n",
       "3   yay,you,both,doing,fine,working,mba,design,str...          0\n",
       "4   you,have,everything,gain,incredib1e,gains,leng...          1\n",
       "..                                                ...        ...\n",
       "67  scifinance,now,automatically,generates,gpu,ena...          0\n",
       "68  you,have,everything,gain,incredib1e,gains,leng...          1\n",
       "69                              will,there,the,latest          0\n",
       "70  experience,with,biggerpenis,today,grow,inches,...          1\n",
       "71  that,cold,there,going,retirement,party,are,the...          0\n",
       "\n",
       "[72 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import wordsList and classLists file\n",
    "wordsListcol=[\"wordList\"]\n",
    "wordsList=pd.read_csv('wordsList',sep=\" \",header=None,names=wordsListcol)\n",
    "classListcol=[\"classList\"]\n",
    "classList=pd.read_csv('classList',sep=\" \",header=None,names=classListcol)\n",
    "wordsList[\"classList\"]=classList['classList']\n",
    "wordsList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7cf5fb",
   "metadata": {},
   "source": [
    "### This function it contains a parameters: str_arg that is an example string that the function preprocess it such as everything apart from letters is excluded, multiple spaces are replaced by single space and the the string is converted to lower case. Lastly it returns the preprocessed string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8165b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_string(str_arg):\n",
    "    new_str=re.sub('[^a-z\\s]+',' ',str_arg,flags=re.IGNORECASE) \n",
    "    new_str=re.sub('(\\s+)',' ',new_str) \n",
    "    new_str=new_str.lower() \n",
    "    \n",
    "    return new_str "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3b5c5f",
   "metadata": {},
   "source": [
    "### In this implementation of NaiveBayes class, there contain four functions in it. \n",
    "#### The two major functions in this class are the train and test functions, while the other two functions are to supplement these two major functions, where addToBow supplements the training function and the getExampleProbability supplements the test functions.\n",
    "##### addToBow function is called by the train function, it has two parameters which has example that is  the features variables and dict_index that implies to whioch Bow category the example belongs to that  it splits the given example using space as a tokenizer and adds every tokenized words to its corressponding Bow. \n",
    "##### Train function is the function that trains the NB model where it takes in the features and the label.  \n",
    "#### getExampleProbability is called by the test function, it takes in one parameter, a single test example,  it estimates probability of the given test example so that it can be classified to a class label and returns the probability of test example in all classes.\n",
    "##### Test function determines the probability of eac test example against all classes and predicts the label against which the class probability is maximum . It returns the predictions of test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32f84ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    \n",
    "    def __init__(self,unique_classes):\n",
    "        \n",
    "        self.classes=unique_classes # Constructor passed with unique number of classes of the training set\n",
    "        \n",
    "\n",
    "    def addToBow(self,example,dict_index):\n",
    "\n",
    "        \n",
    "        if isinstance(example,np.ndarray): example=example[0]\n",
    "     \n",
    "        for token_word in example.split(): #for every word in preprocessed example\n",
    "          \n",
    "            self.bow_dicts[dict_index][token_word]+=1 #increment in its count\n",
    "            \n",
    "    def train(self,features,label):\n",
    "        \n",
    "\n",
    "    \n",
    "        self.examples=features\n",
    "        self.label=label\n",
    "        self.bow_dicts=np.array([defaultdict(lambda:0) for index in range(self.classes.shape[0])])\n",
    "        \n",
    "        # Only convert to numpy arrays if initially not passed as numpy arrays - else its a useless recomputation\n",
    "        \n",
    "        if not isinstance(self.examples,np.ndarray): self.examples=np.array(self.examples)\n",
    "        if not isinstance(self.label,np.ndarray): self.label=np.array(self.label)\n",
    "            \n",
    "        # Constructing Bow for each category\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "          \n",
    "            all_cat_examples=self.examples[self.label==cat] #filter all examples of category == cat\n",
    "            \n",
    "            # Get examples preprocessed\n",
    "            \n",
    "            cleaned_examples=[preprocess_string(cat_example) for cat_example in all_cat_examples]\n",
    "            \n",
    "            cleaned_examples=pd.DataFrame(data=cleaned_examples)\n",
    "            \n",
    "            # Now costruct Bow of this particular category\n",
    "            np.apply_along_axis(self.addToBow,1,cleaned_examples,cat_index)\n",
    "            \n",
    "                \n",
    "        ###################################################################################################\n",
    "        \n",
    "        '''\n",
    "            \n",
    "            We are done with constructing of Bow for each category. But there is a need to precompute a few \n",
    "            other calculations at training time too:\n",
    "            1. prior probability of each class - p(c)\n",
    "            2. vocabulary |V| \n",
    "            3. denominator value of each class - [ count(c) + |V| + 1 ] \n",
    "            \n",
    "            ---------------------\n",
    "            All these 3 calculations can be done at test time too however doing so means to re-compute these \n",
    "            again and again every time the test function will be called - this would significantly\n",
    "            increase the computation time especially when there is a lot of test examples to classify.  \n",
    "            Which does not make sense to repeatedly compute the same thing.\n",
    "            So precompute all of them & use them during test time to speed up predictions.\n",
    "            \n",
    "        '''\n",
    "        \n",
    "        ###################################################################################################\n",
    "      \n",
    "        prob_classes=np.empty(self.classes.shape[0])\n",
    "        all_words=[]\n",
    "        cat_word_counts=np.empty(self.classes.shape[0])\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "           \n",
    "            # Calculating prior probability p(c) for each class\n",
    "            prob_classes[cat_index]=np.sum(self.label==cat)/float(self.label.shape[0]) \n",
    "            \n",
    "            # Calculating total counts of all the words of each class \n",
    "            count=list(self.bow_dicts[cat_index].values())\n",
    "            cat_word_counts[cat_index]=np.sum(np.array(list(self.bow_dicts[cat_index].values())))+1\n",
    "            \n",
    "            # Get all words of this category                                \n",
    "            all_words+=self.bow_dicts[cat_index].keys()\n",
    "                                                     \n",
    "        \n",
    "        # Combine all words of every category & make them unique to get vocabulary -V- of entire training set\n",
    "        \n",
    "        self.vocab=np.unique(np.array(all_words))\n",
    "        self.vocab_length=self.vocab.shape[0]\n",
    "                                  \n",
    "        # Computing denominator value                                      \n",
    "        denoms=np.array([cat_word_counts[cat_index]+self.vocab_length+1 for cat_index,cat in enumerate(self.classes)])                                                                          \n",
    "      \n",
    "        '''\n",
    "            Now that have everything precomputed as well, organize everything in a tuple \n",
    "            rather than to have a separate list for every thing.\n",
    "            \n",
    "            Every element of self.cats_info has a tuple of values\n",
    "            Each tuple has a dict at index 0, prior probability at index 1, denominator value at index 2\n",
    "        '''\n",
    "        \n",
    "        self.cats_info=[(self.bow_dicts[cat_index],prob_classes[cat_index],denoms[cat_index]) for cat_index,\n",
    "                        cat in enumerate(self.classes)]                               \n",
    "        self.cats_info=np.array(self.cats_info)                                 \n",
    "                                              \n",
    "                                              \n",
    "    def getExampleProbability(self,test_example):                                \n",
    "        \n",
    "\n",
    "                                 \n",
    "                                              \n",
    "        likelihood_prob=np.zeros(self.classes.shape[0]) # Store probability w.r.t each class\n",
    "        \n",
    " \n",
    "        for cat_index,cat in enumerate(self.classes): \n",
    "                             \n",
    "            for test_token in test_example.split(): #split the test example and get p of each test word\n",
    "                           \n",
    "                \n",
    "                #get total count of this test token from it's respective training dict to get numerator value                           \n",
    "                test_token_counts=self.cats_info[cat_index][0].get(test_token,0)+1\n",
    "                \n",
    "                   \n",
    "                test_token_prob=test_token_counts/float(self.cats_info[cat_index][2])                              \n",
    "                \n",
    "\n",
    "                likelihood_prob[cat_index]+=np.log(test_token_prob)\n",
    "                                              \n",
    "        # Estimate of the given example against every class but we need posterior probility\n",
    "        post_prob=np.empty(self.classes.shape[0])\n",
    "        for cat_index,cat in enumerate(self.classes):\n",
    "            post_prob[cat_index]=likelihood_prob[cat_index]+np.log(self.cats_info[cat_index][1])                                  \n",
    "      \n",
    "        return post_prob\n",
    "    \n",
    "   \n",
    "    def test(self,test_set):\n",
    "      \n",
    "       \n",
    "        predictions=[] # Store prediction of each test example\n",
    "        for example in test_set: \n",
    "                                              \n",
    "            # Preprocess the test example the same way we did for training set examples                                 \n",
    "            cleaned_example=preprocess_string(example) \n",
    "             \n",
    "            # Get the posterior probability of every example                                  \n",
    "            post_prob=self.getExampleProbability(cleaned_example) #get prob of this example for both classes\n",
    "            \n",
    "            # Pick the max value and map against self.classes\n",
    "            predictions.append(self.classes[np.argmax(post_prob)])\n",
    "                \n",
    "        return np.array(predictions) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02104200",
   "metadata": {},
   "source": [
    "#### Use StratifiedShuffleSplit from sklearn MLlib and split it into 66 lines out of 72 for train and 6 lines for test which is about 93% for train.  X is features and y is the target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1a3a0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=wordsList.wordList\n",
    "y=wordsList.classList\n",
    "from sklearn.model_selection import StratifiedShuffleSplit \n",
    "\n",
    "splitter=StratifiedShuffleSplit(train_size=0.93,random_state=42)\n",
    "\n",
    "for train,test in splitter.split(X,y):    \n",
    "    X_train_SS = X.iloc[train]\n",
    "    y_train_SS = y.iloc[train]\n",
    "    X_test_SS = X.iloc[test]\n",
    "    y_test_SS = y.iloc[test]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c3a76e",
   "metadata": {},
   "source": [
    "#### Shows that our training dataset and test dataset is stratified as can be seen that both train train and test have the same split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c5fad91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_SS no. of rows: 66\n",
      "X_test_SS no. of rows: 6\n",
      "y_train_SS no. of rows: 66\n",
      "y_test_SS no. of rows: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_SS no. of rows: \"+ str(len(X_train_SS)))\n",
    "print(\"X_test_SS no. of rows: \"+ str(len(X_test_SS)))\n",
    "print(\"y_train_SS no. of rows: \"+ str(len(y_train_SS)))\n",
    "print(\"y_test_SS no. of rows: \"+ str(len(y_test_SS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f3398f",
   "metadata": {},
   "source": [
    "#### Train model using Naive Bayes class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "174b6a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=NaiveBayes(np.unique(y_train_SS))\n",
    "nb.train(X_train_SS,y_train_SS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c75bc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction=nb.test(X_test_SS)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7b842e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy:  100.0 %\n"
     ]
    }
   ],
   "source": [
    "# Accuracy of the model\n",
    "test_acc=np.sum(prediction==y_test_SS)/float(y_test_SS.shape[0]) \n",
    "print (\"Test Set Accuracy: \",test_acc*100,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
